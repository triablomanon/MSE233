{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/triablomanon/MSE233/blob/HW4/MSE233_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB2JxZRA8Aw_"
      },
      "source": [
        "# Problem 1. Computing equiblibrium for Prisoner's Dilemma using EXP3 (MWU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-wyInLxBeDF"
      },
      "source": [
        "Consider the prisoner's dilema game from [lecture 2 notes](https://www.vsyrgkanis.com/6853sp19/lec2.pdf) for suspect X and suspect Y defined by the payoff matrices:\n",
        "\\begin{align}\n",
        "X = \\begin{pmatrix} -1/2 & -10\\\\ 0 & -5\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "Y = \\begin{pmatrix} -1/2 & 0\\\\ -10 & -5\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "As described in lecture notes the pure Nash Equilibrium is when both prisoners choose the second action available to them (betray). Note importantly if these are the payoff matrices, then the loss matrices will be the complements (i.e. -X and -Y)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmQDxTcrL-Xp"
      },
      "source": [
        "Implement exponential weight updates algorithm and see if using no-regret dynamics where both players use the EXP algorithm to update their choice probabilities at each step, converges to an approximate Nash equilibrium of the prisoner's dilemma game. Use the template code we provide below and fill in the parts that we left as `{{FILL IN}}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jVKmchei3jA_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU_nrbVi205q"
      },
      "source": [
        "Here we define a generic two player game interface so we can resuse it throughout the homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "agYOeV0877ho"
      },
      "outputs": [],
      "source": [
        "class TwoPlayerGameInterface:\n",
        "    \"\"\"\n",
        "    Generic Interface for a two player game\n",
        "    _numActions : no. of actions for each of the players\n",
        "    _numPlayers : no. of players in the game\n",
        "    _X : loss matrix for x player\n",
        "    _Y : loss matrix for y player\n",
        "    \"\"\"\n",
        "    def __init__(self, X, Y):\n",
        "        self._numActions = X.shape\n",
        "        self._numPlayers = len(self._numActions)\n",
        "        self._X = X\n",
        "        self._Y = Y\n",
        "        print(f'Created a game with {self._numPlayers} players',\n",
        "              f'and number of actions: {self._numActions}')\n",
        "\n",
        "    def getPlayerLoss(self, i : int):\n",
        "        if i == 0:\n",
        "            return self._X\n",
        "        if i == 1:\n",
        "            return self._Y\n",
        "\n",
        "    def getNumActions(self):\n",
        "        return self._numActions\n",
        "\n",
        "    def getNumPlayers(self):\n",
        "        return self._numPlayers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RCIJ6OB3r-H"
      },
      "source": [
        "## Problem 1.1\n",
        "This is the exponential weight updates algorithm as in HW1. Everything is very similar to HW1, we just want you to modify it slightly to generalize it for any two player game with two different loss matrices, instead of a zero-sum game as in HW1. Moreover, instead of keeping track at every period of the distribution of actions of the players, we will be sampling a strategy based on the distribution and giving feedback to the players based on the sampled strategies, and not the expected feedback, over the distribution of the opponent's strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NOIfQEwt8rX1"
      },
      "outputs": [],
      "source": [
        "def playNoRegretGame(twoPlayerGame):\n",
        "    \"\"\"\n",
        "    (1.5 points)\n",
        "    Generic No Regret EXP algorithm for a two player game, similar to HW1\n",
        "    but not necessarily for a zero sum game.\n",
        "    \"\"\"\n",
        "\n",
        "    # number of actions of each player\n",
        "    nActions = twoPlayerGame.getNumActions()\n",
        "\n",
        "    # numberof iterations of no-regret dynamics\n",
        "    T = 10000\n",
        "    # step size for exponential weights\n",
        "    etas = 10 * np.sqrt(np.log(nActions)/(2 * T))\n",
        "\n",
        "    # initialize to random initial probabilities over actions to avoid symmetry\n",
        "    x = np.random.uniform(.5, 1.5, size=nActions[0]) * np.ones((T,nActions[0]))\n",
        "    x /= np.sum(x, axis=-1, keepdims=True)\n",
        "    y = np.random.uniform(.5, 1.5, size=nActions[1]) * np.ones((T,nActions[1]))\n",
        "    y /= np.sum(y, axis=-1, keepdims=True)\n",
        "\n",
        "    # Get player LOSS matrices\n",
        "    X = twoPlayerGame.getPlayerLoss(0)\n",
        "    Y = twoPlayerGame.getPlayerLoss(1)\n",
        "\n",
        "    strategyX, strategyY = np.zeros(T-1).astype(int), np.zeros(T-1).astype(int)\n",
        "    for t in np.arange(1, T):\n",
        "        # sample an action for the x and y player\n",
        "        strategyX[t-1] = np.random.choice(nActions[0], p=x[t-1])\n",
        "        strategyY[t-1] = np.random.choice(nActions[1], p=y[t-1])\n",
        "\n",
        "        # loss vector for x player, given last choice of y\n",
        "        lx = X[:,strategyY[t-1]]\n",
        "        # loss vector for y player, given last choice of x\n",
        "        ly = Y[strategyX[t-1],:]\n",
        "\n",
        "        # update probabilities for x player, based on Exponential Weight Updates\n",
        "        x[t] = x[t-1] * np.exp(-etas[0] * lx) / np.sum(x[t-1] * np.exp(-etas[0] * lx))\n",
        "        # update probabilities for y player, based on Exponential Weight Updates\n",
        "        y[t] = y[t-1] * np.exp(-etas[1] * ly) / np.sum(y[t-1] * np.exp(-etas[1] * ly))\n",
        "\n",
        "    return strategyX, strategyY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QDwlW_PISEoN"
      },
      "outputs": [],
      "source": [
        "def run_prisoners_dilemma_game(X, Y, TwoPlayerGameInterface=TwoPlayerGameInterface, playNoRegretGame=playNoRegretGame):\n",
        "  '''\n",
        "  (0.5 point)\n",
        "  Run the prisoner's dilemma game by using TwoPlayerGameInterface and playNoRegretGame.\n",
        "  Inputs: X,Y: loss matrices\n",
        "  Returns: xbar, ybar: equilibrim startegy for x and y players\n",
        "  '''\n",
        "  prisonersDilemmaGame = TwoPlayerGameInterface(X, Y)\n",
        "  strategyX, strategyY = playNoRegretGame(prisonersDilemmaGame)\n",
        "\n",
        "  # calculate equilibrium as the pair of the empirical distribution of\n",
        "  # choices of x player and the empirical distribution of choices of the y player\n",
        "  # See rho^T in slide 61 of Lecture 7\n",
        "  xbar = np.zeros(X.shape[0])\n",
        "  ybar = np.zeros(Y.shape[1])\n",
        "  for i in strategyX:\n",
        "    xbar[i] += 1\n",
        "  for i in strategyY:\n",
        "    ybar[i] += 1\n",
        "  xbar /= len(strategyX)\n",
        "  ybar /= len(strategyY)\n",
        "  #xbar = np.bincount(strategyX, minlength=X.shape[0]) / len(strategyX)\n",
        "  #ybar = np.bincount(strategyY, minlength=Y.shape[1]) / len(strategyY)\n",
        "  return xbar, ybar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJKypUxjFPWL",
        "outputId": "ac9c74db-86f7-48a4-bd42-cc6e8847928e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a game with 2 players and number of actions: (2, 2)\n",
            "P1 distribution of strategies: [0. 1.]\n",
            "P2 distribution of strategies: [0.001 0.999]\n"
          ]
        }
      ],
      "source": [
        "# LOSS matrix for player x\n",
        "X = np.array([[1/2, 10], [0, 5]])\n",
        "\n",
        "# LOSS matrix for player y\n",
        "Y = np.array([[1/2, 0], [10, 5]])\n",
        "\n",
        "xbar, ybar = run_prisoners_dilemma_game(X, Y)\n",
        "\n",
        "with np.printoptions(precision=3, suppress=True):\n",
        "    print(f\"P1 distribution of strategies: {xbar}\")\n",
        "    print(f\"P2 distribution of strategies: {ybar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9WBKrk0SR9L"
      },
      "source": [
        "Expected answer: \\\n",
        "P1 distribution of strategies: [0. 1.] \\\n",
        "P2 distribution of strategies: [0.001 0.999]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhn208rH4o91"
      },
      "source": [
        "(0.5 points) Explain whether the marginal empirical distributions of each player's choice probabilities converged to a Nash equilibrium of the prisoner's dilemma game?\n",
        "\n",
        "Given the loss matrices for the two players, we can see that the marginal empirical distributions of the player's choice probabilities **DID converge** to a Nash equilibrium:\n",
        "\n",
        "Indeed for Player 2, strategy 2 is dominant:\n",
        "- loss 0 vs. 1/2 when P1 plays strategy 1\n",
        "- loss 5 vs 10 when P1 plays strategy 2\n",
        "\n",
        "Therefore, [0, 1] for both P1 and P2 is a Nash equilibrium, and the algorithm has reached a distribution of [0, 1] for P1 and [0.001, 0.999] for P2, which is satisfyingly close.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P3JBZll666h"
      },
      "source": [
        "# Problem 2. No-Swap Regret Dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC98B3wOdKmG"
      },
      "source": [
        "## Problem 2.1 Implementing No-Swap Regret Dynamics\n",
        "\n",
        "In Lectures 7 & 8, we explored how no-swap regret dynamics are stronger than no-regret dynamics. The joint empirical distribution of no-regret dynamics rapidly converges to coarse-correlated equilibria in arbitrary games (see e.g. the [Lecture Notes](https://www.vsyrgkanis.com/6853sp19/lecture06.pdf) and the corresponding class [Lecture 7, Presentation](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture7.pdf).  In no-swap regret dynamics, the learning algorithm instead converges to a correlated equilibrium. For details beyond what was covered in [Lecture 7](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture7.pdf). More specifically, no-swap regret describes how we wouldn't prefer a strategy by which we change an action to some other action given some deterministic fixed mapping. Clearly no-regret is weaker than this because no-regret is saying we wouldn't prefer mapping every action we took to a single action, which is encompassed by no-swap. In this part of the assignment we will be implementing no-swap regret dynamics via the no-regret to no-swap regret reduction described in the [lecture notes](https://www.vsyrgkanis.com/6853sp19/lecture06.pdf) and in [Lecture 8](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture8.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhAe2q1mX9Pb"
      },
      "source": [
        "### Setting up the no-swap no-regret reduction\n",
        "\n",
        "The following code sets up the no-swap no-regret reduction setting up each player's choice probabilities (playerStrategies) and the choice probabilies for each no-regret algorithm that each player has access to (algoStrategies). Note that the number of algorithms each player has is equal to the number of actions available to them. You should read this code to understand how each of these data structures is initailized but DO NOT MODIFY THIS CODE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbeK1k5Rq7a5"
      },
      "outputs": [],
      "source": [
        "# Initializing RPS (Rock-Paper-Scissors) game\n",
        "def initializeRPSGame(rpsGame : TwoPlayerGameInterface):\n",
        "    '''\n",
        "    Create initiatlization for an Rock-Paper-Scissors (RPS) game.\n",
        "    returns:\n",
        "    T: total # of time periods\n",
        "    etas: learning constant\n",
        "    algoStrategies: algorithm strategy initialization\n",
        "    nActions: # of actions\n",
        "    nplayers: # of players\n",
        "    '''\n",
        "    # Note that algoStrategies[0] entries are the algorithm weights for player 1\n",
        "    # for each time t. algoStrategies[1] entries are the corresponding\n",
        "    # for player 2.\n",
        "    # algoStrategies[0][0] for example are player 1's algorithm weights\n",
        "    # at time t = 0.\n",
        "    T = 10000 # number of learning periods\n",
        "    algoStrategies, playerStrategies = {}, {}\n",
        "    nActions = rpsGame.getNumActions()\n",
        "    nplayers = rpsGame.getNumPlayers()\n",
        "\n",
        "    etas = 10 * np.sqrt(np.log(nActions) / (2 * T))\n",
        "\n",
        "    for i in range(nplayers):\n",
        "        # weights of all algorithm instances of player i at each learning period\n",
        "        # we add some randomness in the initialization to break the symmetry\n",
        "        # of the dynamics\n",
        "        rand_init = np.random.uniform(.5, 1.5, size=(nActions[i], nActions[i]))\n",
        "        algoStrategies[i] = rand_init * np.ones((T, nActions[i], nActions[i]))\n",
        "        algoStrategies[i] /= np.sum(algoStrategies[i], axis=-1, keepdims=True)\n",
        "\n",
        "    return T, etas, algoStrategies, nActions, nplayers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd4KHAs8dNVe"
      },
      "outputs": [],
      "source": [
        "def solveForQ(weights):\n",
        "    \"\"\"\n",
        "    Determines the player's probability distribution by finding the eigenvectors\n",
        "    of the player's algorithm weights. Note an eigenvector of a matrix is a vector\n",
        "    that when multiplied by a matrix, yields itself multiplied by some scalar\n",
        "    (the eigenvalue). Here we return the eigenvector whose eigenvalue is 1.\n",
        "    \"\"\"\n",
        "    evals, evecs = np.linalg.eig(weights.T)\n",
        "    evec1 = evecs[:, np.isclose(evals, 1)]\n",
        "    evec1 = evec1[:, 0]\n",
        "    # eigs finds complex eigenvalues and eigenvectors, so you want the real part\n",
        "    real_evec1 = np.abs(evec1.real)\n",
        "    stationary = real_evec1 / np.sum(real_evec1)\n",
        "    return stationary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vO4DneuXbi9"
      },
      "source": [
        "### No-swap regret dynamics algorithm\n",
        "Implement the black box reduction of no-regret to no-swap regret. Use multiplicative weight updates and compute an approximate equilibrium of the game using no-swap regret dynamics where both players use the MWU algorithm to update their choice probabilities of their no-regret algorithms at each step.\n",
        "\n",
        "Consult Slides 61-64 from [Lecture 8](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture8.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSu7pxtSbvCb"
      },
      "outputs": [],
      "source": [
        "def playNoSwapRegretGame(rpsGame : TwoPlayerGameInterface):\n",
        "    '''\n",
        "    (3 points)\n",
        "    Implement the no-swap-regret algorithm for the rps game defined by a TwoPlayerGameInterface.\n",
        "    Returns: strategyX, strategyY: equilibrium strategies for players x and y\n",
        "    '''\n",
        "    T, etas, algoStrategies, nActions, nPlayers = initializeRPSGame(rpsGame)\n",
        "    strategyX, strategyY = np.zeros(T-1).astype(int), np.zeros(T-1).astype(int)\n",
        "    # Iterate over learning periods\n",
        "    for t in np.arange(1, T):\n",
        "        # Determine the probability distribution q over actions for each player\n",
        "        qX = solveForQ(algoStrategies[0][t-1])\n",
        "        qY = solveForQ(algoStrategies[1][t-1])\n",
        "\n",
        "        # Pick action to play for player 1 based on player weights\n",
        "        strategyX[t-1] = np.random.choice(nActions[0], p=qX)\n",
        "        # Pick action to play for player 2 based on player weights\n",
        "        strategyY[t-1] = np.random.choice(nActions[1], p=qY)\n",
        "\n",
        "        # Get the loss matrix for player 1\n",
        "        lossMatrixX = rpsGame.getPlayerLoss(0)\n",
        "        # Get the payoff from opponent's action\n",
        "        actionLossX = lossMatrixX[:, strategyY[t-1]]\n",
        "        # Update the algorithm weights based on loss and player strategies\n",
        "        for j in np.arange(nActions[0]):\n",
        "            # calculate perceived loss vector for the Aj instance\n",
        "            # of the EXP algorithm\n",
        "            perceivedLossXj = actionLossX * qX[j]\n",
        "            # update the Aj algorithm weights using the EXP rule\n",
        "            algoStrategies[0][t, j, :] = algoStrategies[0][t-1, j, :] * np.exp(-etas[0] * perceivedLossXj) / np.sum(algoStrategies[0][t-1, j, :] * np.exp(-etas[0] * perceivedLossXj))\n",
        "\n",
        "\n",
        "        # Get the loss matrix for player 1\n",
        "        lossMatrixY = rpsGame.getPlayerLoss(1)\n",
        "        # Get the payoff from opponent's action\n",
        "        actionLossY = lossMatrixY[strategyX[t-1], :]\n",
        "\n",
        "        # Update the algorithm weights based on loss and player strategies\n",
        "        for j in np.arange(nActions[1]):\n",
        "            # calculate perceived loss vector for the Aj instance\n",
        "            # of the EXP algorithm\n",
        "            perceivedLossYj = actionLossY * qY[j]\n",
        "            # update the Aj algorithm weights using the EXP rule\n",
        "            algoStrategies[1][t, j, :] = algoStrategies[1][t-1, j, :] * np.exp(-etas[1] * perceivedLossYj) / np.sum(algoStrategies[1][t-1, j, :] * np.exp(-etas[1] * perceivedLossYj))\n",
        "\n",
        "    return strategyX, strategyY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVfcV5ekZj7c"
      },
      "source": [
        "### Problem 2.2 Calculate the empirical joint distribution (equilibrium).\n",
        "\n",
        "\n",
        "Using the choice probabilies of each player, determine empirical joint distribution (i.e. the quantity $\\pi^T$ in [Lecutre 7](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture7.pdf), Slides 61). This distribution should be an approximate Correlated Equilibrium (see [Lecture 8, Slide 66](https://raw.githubusercontent.com/stanford-msande233/spring25/master/assets/presentations/Lecture8.pdf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCsGkvojxsFt"
      },
      "outputs": [],
      "source": [
        "def calculateEquilibrium(rpsGame : TwoPlayerGameInterface, strategyX, strategyY):\n",
        "    \"\"\"\n",
        "    (1 point)\n",
        "    QJoint represents the mean likelihood of action x,y from player X,Y.\n",
        "    Moreover for a 2 player game with finite actions (N,M) for player X and\n",
        "    player Y respecitively. QJoint will be an NxM matrix i.e. same size as\n",
        "    the payoff matrix.\n",
        "    \"\"\"\n",
        "    # Determining the shape of the joint distribution from the loss matrix\n",
        "    qjoint = np.zeros(rpsGame.getPlayerLoss(0).shape)\n",
        "    for index in np.ndindex(qjoint.shape):\n",
        "        # Determine the empirical probability of action pairs of the two players\n",
        "        # Note index is a the tuple (x,y) from (0,0) -> (n,m)\n",
        "        n, m = index\n",
        "        qjoint[index] = np.mean((strategyX == n) & (strategyY == m))\n",
        "    return qjoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX-Sei5hZ5fe"
      },
      "source": [
        "### Problem 2.3\n",
        "Calculate the swap regret at the calculated Approximate Correlated Equilibrium  for each pair of strategies $j,j'$ of each player. Consult with [Lecture 8, Slide 66](https://raw.githubusercontent.com/stanford-msande233/spring24/master/assets/presentations/Lecture8.pdf). Then calculate the maximum such regret over deviating strategies $j'$ for each action $j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M32UnHCGMbo0"
      },
      "outputs": [],
      "source": [
        "def calculateSwapRegret(rpsGame : TwoPlayerGameInterface, qjoint):\n",
        "    '''\n",
        "    (2 points)\n",
        "    Compute and print the swap regret given an instance of a game defined by TwoPlayerGameInterface\n",
        "    and qjoint returned by calculateEquilibrium.\n",
        "    '''\n",
        "\n",
        "    # get the loss matrices for each player and the number of actions of each\n",
        "    # player\n",
        "    lossX = rpsGame.getPlayerLoss(0)\n",
        "    lossY = rpsGame.getPlayerLoss(1)\n",
        "    n, m = rpsGame.getNumActions()\n",
        "\n",
        "    # Calculate and print swap regrets for player X\n",
        "    for j in range(n):\n",
        "        # loss for player x when playing algorithm j\n",
        "        loss_j_x = np.sum(lossX[j, :] * qjoint[j, :])\n",
        "        max_regret_j_x = -np.inf\n",
        "        for jp in range(n):\n",
        "            loss_jp_x = np.sum(lossX[jp, :] * qjoint[j, :])\n",
        "            regret_j_jp_x = loss_j_x - loss_jp_x\n",
        "            max_regret_j_x = max(max_regret_j_x, regret_j_jp_x)\n",
        "        print(f\"Player X, Action {j}: {max_regret_j_x:.4f}\")\n",
        "\n",
        "\n",
        "    # Calculate and print swap regrets for player Y\n",
        "    for j in range(m):\n",
        "        # loss for player y when playing algorithm j\n",
        "        loss_j_y = np.sum(lossY[:, j] * qjoint[:, j])\n",
        "        max_regret_j_y = -np.inf\n",
        "        for jp in range(m):\n",
        "            loss_jp_y = np.sum(lossY[:, jp] * qjoint[:, j])\n",
        "            regret_j_jp_y = loss_j_y - loss_jp_y\n",
        "            max_regret_j_y = max(max_regret_j_y, regret_j_jp_y)\n",
        "        print(f\"Player Y, Action {j}: {max_regret_j_y:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEgXLrd8eXwz"
      },
      "source": [
        "## Problem 2.4. Regular Rock-Paper-Scissors (RPS) Game\n",
        "\n",
        "For regular RPS we have a simple payoff bi-matrix (which we define as two separate matrices X & Y) for player x and player y. Here rock, paper, scissors correspond to indices 0, 1, 2 respectively. Observe that this is a zero-sum game with payoff matrices\n",
        "\n",
        "\\begin{align}\n",
        "X = \\begin{pmatrix} 0 & -1 & 1\\\\ 1 & 0 & -1\\\\ -1 & 1 & 0\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "Y = \\begin{pmatrix} 0 & 1 & -1\\\\ -1 & 0 & 1\\\\ 1 & -1 & 0\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In this section we compare the no-swap regret dynamics of the regular RPS game to the no-regret dyamics. Note that this game DOES converge to a correlated equilibrium!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RptmUKTGe1Kl"
      },
      "source": [
        "### First we define the regularRPS game using our TwoPlayerGameInterface\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBhCCHeCesWl",
        "outputId": "a1f7e417-5c8a-4f6e-98df-ca848d619208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a game with 2 players and number of actions: (3, 3)\n"
          ]
        }
      ],
      "source": [
        "# Define the regular RPS game using loss matrices (negative of payoff matrices)\n",
        "regularRPSXLoss = - np.array([[0, -1,  1],\n",
        "                              [1,  0, -1],\n",
        "                              [-1, 1,  0]])\n",
        "regularRPSYLoss = - regularRPSXLoss\n",
        "regularRPSGame = TwoPlayerGameInterface(regularRPSXLoss, regularRPSYLoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUSk-G7AE6W1"
      },
      "source": [
        "### Analysis of the no-swap regret dynamics v.s. no-regret dynamics of regular RPS game.\n",
        "In this section we compare the no-swap regret dynamics on the Regular RPS game to the no-regret dynamics of the Regular RPS game.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0siqiLTrc5re"
      },
      "source": [
        "#### No-swap regret Dynamics of Regular RPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRYE1oeavSx_",
        "outputId": "3a0ea964-5353-42e1-f9d6-bb9e22dffffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical joint Distribution:\n",
            " [[0.116 0.112 0.111]\n",
            " [0.108 0.114 0.111]\n",
            " [0.109 0.11  0.109]]\n",
            "\n",
            "Swap regrets are:\n",
            "Player X, Action 0: 0.0050\n",
            "Player X, Action 1: 0.0097\n",
            "Player X, Action 2: 0.0000\n",
            "Player Y, Action 0: 0.0052\n",
            "Player Y, Action 1: 0.0003\n",
            "Player Y, Action 2: 0.0022\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(123567)\n",
        "\n",
        "# Play Regular RPS game with No Swap Regret Dynamics\n",
        "regularRPSactionsX, regularRPSactionsY = playNoSwapRegretGame(regularRPSGame)\n",
        "\n",
        "# calculate empirical joint distribution\n",
        "qjoint = calculateEquilibrium(regularRPSGame, regularRPSactionsX, regularRPSactionsY)\n",
        "with np.printoptions(precision=3):\n",
        "    print(f\"Empirical joint Distribution:\\n {qjoint}\\n\")\n",
        "\n",
        "# The empirical joint distribution should be an approximate correlated\n",
        "# equilibrium. We verify that by calculating the swap regret\n",
        "# using the function you wrote earlier\n",
        "print('Swap regrets are:')\n",
        "calculateSwapRegret(regularRPSGame, qjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHvRHTCnmGm6"
      },
      "source": [
        "Expected answers: \\\n",
        "Empirical joint Distribution: \\\n",
        " [[0.116 0.112 0.111] \\\n",
        " [0.108 0.114 0.111] \\\n",
        " [0.109 0.11  0.109]] \\\n",
        "\n",
        "Swap regrets are \\\n",
        "Player X, Action 0: 0.0050 \\\n",
        "Player X, Action 1: 0.0097 \\\n",
        "Player X, Action 2: 0.0000 \\\n",
        "Player Y, Action 0: 0.0052 \\\n",
        "Player Y, Action 1: 0.0003 \\\n",
        "Player Y, Action 2: 0.0022 \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol3vTwuWEWit"
      },
      "source": [
        "#### No-regret Dynamics of Regular RPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpwIzk_yf7bW",
        "outputId": "ae811507-3a24-4d23-c13d-9851c5dbf0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical joint Distribution:\n",
            " [[0.011 0.165 0.162]\n",
            " [0.158 0.011 0.161]\n",
            " [0.155 0.167 0.011]]\n",
            "\n",
            "Swap regrets are:\n",
            "Player X, Action 0: 0.1575\n",
            "Player X, Action 1: 0.1528\n",
            "Player X, Action 2: 0.1310\n",
            "Player Y, Action 0: 0.1499\n",
            "Player Y, Action 1: 0.1584\n",
            "Player Y, Action 2: 0.1519\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(123567)\n",
        "\n",
        "# Play Regular RPS game with No Regret Dynamics\n",
        "regularRPSactionsX, regularRPSactionsY = playNoRegretGame(regularRPSGame)\n",
        "\n",
        "# calculate empirical joint distribution\n",
        "qjoint = calculateEquilibrium(regularRPSGame, regularRPSactionsX, regularRPSactionsY)\n",
        "with np.printoptions(precision=3):\n",
        "    print(f\"Empirical joint Distribution:\\n {qjoint}\\n\")\n",
        "\n",
        "# Calculate the swap regret using the function you wrote earlier\n",
        "print('Swap regrets are:')\n",
        "calculateSwapRegret(regularRPSGame, qjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVsDgZu9moUp"
      },
      "source": [
        "Expected Answers: \\\n",
        "\n",
        "Empirical joint Distribution: \\\n",
        " [[0.011 0.165 0.162] \\\n",
        " [0.158 0.011 0.161] \\\n",
        " [0.155 0.167 0.011]] \\\n",
        "\n",
        "Swap regrets are:  \\\n",
        "Player X, Action 0: 0.1575 \\\n",
        "Player X, Action 1: 0.1528 \\\n",
        "Player X, Action 2: 0.1310 \\\n",
        "Player Y, Action 0: 0.1499 \\\n",
        "Player Y, Action 1: 0.1584 \\\n",
        "Player Y, Action 2: 0.1519 \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGLlsKf9gA60"
      },
      "source": [
        "### Question:\n",
        "(1 point) Comment on the differences between the swap-regret when running no-swap regret dynamics and no-regret dynamics for regular RPS. Does the empirical joint distribution converge to a correlated equilibrium under the swap-regret dynamics? Does the empirical joint distribution converge to a correlated equilibrium under the no-regret dynamics? Do they converge to some other relaxed notion of a correlated equilibrium?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY1tRATwg2Y0"
      },
      "source": [
        "###Answer\n",
        "\n",
        "We can see that as we expect, running the no-swap regret dynamics leads to swap regrets that are much closer to 0 than running the standard no-regret dynamics for the Rock Paper Scissors game.\n",
        "\n",
        "We also see that the no-regret dynamics do not lead to a correlated equilibrium for the empirical joint distribution. For example for Player 1 starting with action 0 (Rock):\n",
        "\n",
        "Empirical joint Distribution:\n",
        " [[0.011 0.165 0.162]\n",
        " [0.158 0.011 0.161]\n",
        " [0.155 0.167 0.011]]\n",
        "\n",
        "Switching the actions 0 (Rock) to action 2 (Scissors) would lead to a gain in utility of -1\\*0.011 + 1\\*0.165 + 0*0.162 ~ 0.154 > 0\n",
        "\n",
        "So by checking this one swap we can see that we are very far from a correlated equilibrium.\n",
        "\n",
        "From lecture 7 we know that we instead reach a **coarse correlated equilibrium** where the expected utility over all swaps from a given action converges to 0, but not individual swaps."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cHJt4uGwilM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}